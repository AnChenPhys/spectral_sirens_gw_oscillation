{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral siren cosmology\n",
    "\n",
    "In this notebook we perform a hierarchical Bayesian inference to constrain both cosmology and astrophysics. We *assume* a paramewtric model in boith cases. \n",
    "\n",
    "We convert our previous code to numpyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Numpyro and friends\n",
    "import numpyro\n",
    "from numpyro.infer import NUTS,MCMC\n",
    "import numpyro.distributions as dist\n",
    "import jax\n",
    "from jax import random\n",
    "from jax import jit\n",
    "from jax.config import config\n",
    "import jax.numpy as jnp\n",
    "from jax.scipy.special import erf\n",
    "import arviz as az\n",
    "import h5py\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "#PYTHON MODULES\n",
    "from constants import *\n",
    "import jgwcosmo\n",
    "import jgwpop\n",
    "import jgw_utils as jgw\n",
    "import likelihood as lik\n",
    "\n",
    "#Directories\n",
    "dir_plots=\"plots_spectral_sirens/\"\n",
    "dir_inj = 'data_injections/'\n",
    "dir_mock = 'data_mock_catalogues/'\n",
    "\n",
    "#PLOTS\n",
    "import matplotlib.pyplot as plt\n",
    "import corner\n",
    "\n",
    "#from matplotlib.ticker import ScalarFormatter\n",
    "#%config InlineBackend.figure_format = 'retina'\n",
    "#plt.rc('text', usetex=True)\n",
    "#plt.rc('font', family='serif')\n",
    "fontSz = 15\n",
    "fontsz = 13\n",
    "fontssz = 11\n",
    "\n",
    "new_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728',\n",
    "              '#9467bd', '#8c564b', '#e377c2', '#7f7f7f',\n",
    "              '#bcbd22', '#17becf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We load our fiducial universe parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiducial_universe import *\n",
    "model_name = 'powerlaw_peak'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Priors\n",
    "log10r0_min, log10r0_max = -3., 3.\n",
    "h0_min, h0_max = 0.4, 1.2\n",
    "Om0_min, Om0_max = 0.1, .6\n",
    "alpha_min, alpha_max = 0., 25.\n",
    "kappa1_min, kappa1_max = -4., 12.\n",
    "kappa2_min, kappa2_max = -4., 12.\n",
    "mmin_min, mmin_max = 1., 20.\n",
    "mmax_min, mmax_max = 30., 100.\n",
    "b_min, b_max = 0., 1.\n",
    "beta_min, beta_max = 0., 10.\n",
    "zp_min, zp_max = 0., 4.\n",
    "gamma_min, gamma_max = -10, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = 'O5'\n",
    "params = 'm1z_m2z_dL'\n",
    "Ndet = 7178982\n",
    "Ndraw = 10000000\n",
    "\n",
    "#Reading the data\n",
    "data = h5py.File(dir_inj+'injections_'+detector+'_'+params+'_Ndraws_%s_Ndet_%s.hdf5' % (Ndraw,Ndet), \"r\")\n",
    "m1z_inj = np.array(data['m1z_inj'])\n",
    "m2z_inj = np.array(data['m2z_inj'])\n",
    "dL_inj = np.array(data['dL_inj'])\n",
    "p_draw_inj = np.array(data['p_draw_inj'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Mock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20\n",
    "n_detections = 1001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load mock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1z_mock_samples = np.load('data_mock_catalogues/m1z_'+detector+'_Ndet_%s_Nsamples_%s_' % (n_detections,n_samples)+model_name+'.npy')\n",
    "m2z_mock_samples = np.load('data_mock_catalogues/m2z_'+detector+'_Ndet_%s_Nsamples_%s_' % (n_detections,n_samples)+model_name+'.npy')\n",
    "dL_mock_samples = np.load('data_mock_catalogues/dL_'+detector+'_Ndet_%s_Nsamples_%s_' % (n_detections,n_samples)+model_name+'.npy')\n",
    "pdraw_mock_samples = np.load('data_mock_catalogues/pdraw_'+detector+'_Ndet_%s_Nsamples_%s_' % (n_detections,n_samples)+model_name+'.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Likelihood\n",
    "\n",
    "We are solving the hierarchical inference of a Poissonian proccess described by the population distribution\n",
    "\n",
    "$$\\frac{dN(\\lambda)}{d\\theta}=\\frac{p_{pop}(\\theta|\\lambda)}{N}$$\n",
    "\n",
    "where $\\lambda$ are the hyperparameters of the model (power-law+peak + SFR + LCDM in our case) and $\\theta$ the parameters that can be observed ($m_z$ and $d_L$ in our case).\n",
    "\n",
    "The population likelihood is given by\n",
    "\n",
    "$$p(\\lambda)=\\prod_{i}^{N_{obs}}\\frac{\\int d\\theta p(d_i|\\theta_i)p_{pop}(\\theta_i|\\lambda)}{\\int d\\theta p_{det}(\\theta)p_{pop}(\\theta|\\lambda)}\n",
    "\\propto\\prod_{i}^{N_{obs}}\\frac{\\int d\\theta p_{pop}(\\theta_i|\\lambda)p(\\theta_i|d_i/\\pi_{pe}(\\theta)}{\\int d\\theta p_{det}(\\theta)p_{pop}(\\theta|\\lambda)}\n",
    "\\approx \\left(\\frac{N_{det}}{N}\\right)^{-N_{obs}}\\prod_{i}^{N_{obs}}\\left\\langle \\frac{dN/d\\theta}{N \\pi_{pe}}\\right\\rangle_{p(\\theta_i|d)}$$\n",
    "\n",
    "where we have transformed the likelihood of the data into its posterior distribution (with the proper PE prior $\\pi_{pe}(\\theta)$- in our case the weigths from sampling the posteriors in different variables) and we have neglected the evidence of the data $p(d_i)$ that would cancel out when computing the posterior distribution. Then, we have approximated the expression with a Monte Carlo integral over the posterior samples of the data.\n",
    "\n",
    "Here the expected number of detections is\n",
    "\n",
    "$$N_{det} = N \\int d\\theta p_{det}(\\theta)p_{pop}(\\theta|\\lambda) \\approx N \\frac{1}{N_{draw}}\\sum_{j}^{N_{found}}\\frac{dN/d\\theta(\\theta_j|\\lambda)}{p_{draw}(\\theta_j)}$$\n",
    "\n",
    "that can be approximated with an injection campaign to account for the detector's sensitivity.\n",
    "\n",
    "Note that the total rate $N$ cancels in this expression so that\n",
    "\n",
    "$$\\log p(\\lambda)\\approx -N_{obs}\\cdot\\log N_{det} + \\sum_{i}^{N_{obs}}\\log\\left\\langle \\frac{dN/d\\theta}{ \\pi_{pe}}\\right\\rangle_{p(\\theta_i|d)}$$\n",
    "\n",
    "This is not the case however when the selection effects are neglected.\n",
    "\n",
    "When considering the Poisson process with rate $N$ but marginalize over the total rate $N$ assuming a $1/N$ prior, we also get to the same expression modulo an overall constant that does not affect the inference. \n",
    "\n",
    "--\n",
    "Nice references: https://arxiv.org/pdf/1809.02063.pdf, https://arxiv.org/pdf/1904.10879.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*A note on Jacobians*\n",
    "\n",
    "For this particular problem we are using data in detector frame masses and luminosity distances, but we are fitting models in the source masses and redshifts. This means that \n",
    "\n",
    "$$\\frac{dN}{dm_zdd_L}=\\frac{dN}{dmdz}\\left|\\frac{\\partial z}{\\partial d_L}\\right|\\left|\\frac{\\partial m}{\\partial m_z}\\right|$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\frac{\\partial d_L}{\\partial z}= \\frac{d_L}{1+z} + (1+z)\\frac{D_H}{E(z)}$$\n",
    "\n",
    "$$\\frac{\\partial m_z}{\\partial m}= (1 + z)$$\n",
    "\n",
    "If we want to fit in mass ratio then we need to add another Jacobian\n",
    "\n",
    "$$\\frac{\\partial (m_1,q)}{\\partial (m_1,m_2)}= \\frac{1}{m_1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logNdet_events(m1,m2,z,p_draw,H0,Om0,r0,Tobs,zp,alpha_z,beta,mmin,mmax,alpha,sig_m1,mu_m1,f_peak,bq,Nsamples):\n",
    "    #input data (N,M): N detections x M samples\n",
    "    \n",
    "    log_pm1 = jgwpop.logpowerlaw_peak(m1,mmin,mmax,alpha,sig_m1,mu_m1,f_peak)\n",
    "    q = m2/m1\n",
    "    log_pq = jgwpop.logpowerlaw(q,0.,1.,bq)\n",
    "    logJacobian_m1z_m1 = - 1.0*jnp.log1p(z)\n",
    "    logJacobian_m2z_m2 = - 1.0*jnp.log1p(z)\n",
    "    logJacobian_m1m2_m1q =  - jnp.log(m1)\n",
    "    log_pm = log_pm1 + log_pq + logJacobian_m1z_m1 + logJacobian_m2z_m2 + logJacobian_m1m2_m1q\n",
    "    logcosmo = lik.log_cosmo(z,H0,Om0)\n",
    "    logRzs = lik.log_Rz(z,r0,zp,alpha_z,beta) + jnp.log(Tobs)\n",
    "    log_dN = log_pm + logcosmo + logRzs - jnp.log(p_draw)    \n",
    "    \n",
    "    return jnp.sum(jax.scipy.special.logsumexp(log_dN,axis=1) - jnp.log(Nsamples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expecte Ndet with injection sesitivity\n",
    "def logNdet_exp(m1z_inj,m2z_inj,dL_inj,p_draw_inj,Ndraw,H0,Om0,r0,Tobs,zp,alpha_z,beta,mmin,mmax,alpha,sig_m1,mu_m1,f_peak,bq):\n",
    "    m1_inj, m2_inj, z_inj = jgwcosmo.detector_to_source_frame_approx(m1z_inj,m2z_inj,dL_inj,H0,Om0,zmin=1e-3,zmax=100)\n",
    "    \n",
    "    log_pm1 = jgwpop.logpowerlaw_peak(m1_inj,mmin,mmax,alpha,sig_m1,mu_m1,f_peak)\n",
    "    q_inj = m2_inj/m1_inj\n",
    "    log_pq = jgwpop.logpowerlaw(q_inj,0.,1.,bq)\n",
    "    m1s_norm = jnp.linspace(tmp_min,tmp_max,1000)    \n",
    "    norm_m1 = jax.scipy.integrate.trapezoid(jgwpop.powerlaw_peak(m1s_norm,mmin,mmax,alpha,sig_m1,mu_m1,f_peak),m1s_norm)\n",
    "    \n",
    "    logJacobian_m1z_m1 = - 1.0*jnp.log1p(z_inj)\n",
    "    logJacobian_m2z_m2 = - 1.0*jnp.log1p(z_inj)\n",
    "    logJacobian_m1m2_m1q =  - jnp.log(m1_inj)\n",
    "    log_pm = log_pm1 + log_pq - jnp.log(norm_m1) + logJacobian_m1z_m1 + logJacobian_m2z_m2 + logJacobian_m1m2_m1q\n",
    "    \n",
    "    logcosmo = lik.log_cosmo_dL(z_inj,dL_inj,H0,Om0)\n",
    "    logRzs = lik.log_Rz(z_inj,r0,zp,alpha_z,beta) + jnp.log(Tobs)\n",
    "    log_dN = log_pm  + logcosmo + logRzs \n",
    "    \n",
    "    #Expected number of detections\n",
    "    log_N = jax.scipy.special.logsumexp(log_dN - jnp.log(p_draw_inj)) - jnp.log(Ndraw)\n",
    "    \n",
    "    #Effective number of samples\n",
    "    log_N2 = jax.scipy.special.logsumexp(2.0*log_dN - 2.0*jnp.log(p_draw_inj)) - 2.0*jnp.log(Ndraw)\n",
    "    log_sigma2 = lik.logdiffexp(log_N2, 2.0*log_N - jnp.log(Ndraw))\n",
    "    Neff = jnp.exp(2.0*log_N - log_sigma2)\n",
    "    \n",
    "    return log_N, Neff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def log_lik(h0,Om0,mmin,mmax,alpha,mu_m1,sig_m1,f_peak,bq,alpha_z,zp,beta):\n",
    "    #Fixed rate\n",
    "    r0 = 1.0 # 10.**log10r0\n",
    "    Tobs = 1.\n",
    "    #redefinition\n",
    "    H0 = h0*100\n",
    "    \n",
    "    Nobs, Nsamples = jnp.shape(m1z_mock_samples)\n",
    "    \n",
    "    D_H = (Clight/1.0e3)  / H0 #Mpc\n",
    "    m1_mock, m2_mock, z_mock = jgwcosmo.detector_to_source_frame_approx(m1z_mock_samples,m2z_mock_samples,dL_mock_samples,H0,Om0,zmin=1e-3,zmax=100)\n",
    "    \n",
    "    #Log_lik Events\n",
    "    p_draw_mock = pdraw_mock_samples\n",
    "    loglik_E = logNdet_events(m1_mock,m2_mock,z_mock,p_draw_mock,H0,Om0,r0,Tobs,zp,alpha_z,beta,mmin,mmax,alpha,sig_m1,mu_m1,f_peak,bq,Nsamples)\n",
    "\n",
    "    #Total rate normalization only needed when selection effects are neglected because then N does not cancel out (see notes above)\n",
    "    #zs_norm = jnp.linspace(0.01,10,1000) \n",
    "    #dn_detec = jgwpop.rate_z(zs_norm,zp,alpha_z,beta)*jgwcosmo.diff_comoving_volume_approx(zs_norm,H0,Om0)/(1.+zs_norm)   \n",
    "    #norm_z = jax.scipy.integrate.trapezoid(dn_detec,zs_norm)\n",
    "    #loglik_E -= Nobs*jnp.log(norm_z)\n",
    "    \n",
    "    m1s_norm = jnp.linspace(tmp_min,tmp_max,1000)    \n",
    "    norm_m1 = jax.scipy.integrate.trapezoid(jgwpop.powerlaw_peak(m1s_norm,mmin,mmax,alpha,sig_m1,mu_m1,f_peak),m1s_norm)\n",
    "    loglik_E -= Nobs*jnp.log(norm_m1)\n",
    "        \n",
    "    #Selection effects\n",
    "    log_Ndet, Neff = logNdet_exp(m1z_inj,m2z_inj,dL_inj,p_draw_inj,Ndraw,H0,Om0,r0,Tobs,zp,alpha_z,beta,mmin,mmax,alpha,sig_m1,mu_m1,f_peak,bq)\n",
    "    loglik_N = -Nobs*log_Ndet\n",
    "        \n",
    "    return loglik_N + loglik_E, Neff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_probability():\n",
    "    \n",
    "    #Priors\n",
    "    h0 = numpyro.sample(\"h0\",dist.Uniform(h0_min,h0_max))\n",
    "    Om0 = Om0_fid#numpyro.sample(\"Om0\",dist.Uniform(Om0_min,Om0_max))\n",
    "    mmin = numpyro.sample(\"mmin\",dist.Uniform(mmin_min,mmin_max))\n",
    "    mmax = numpyro.sample(\"mmax\",dist.Uniform(mmax_min,mmax_max))\n",
    "    alpha = numpyro.sample(\"alpha\",dist.Normal(0,5))\n",
    "    mu_m1 = numpyro.sample(\"mu_m1\",dist.Uniform(20,60))\n",
    "    sig_m1 = numpyro.sample(\"sig_m1\",dist.Uniform(1,10))\n",
    "    f_peak = numpyro.sample(\"f_peak\",dist.Uniform(0,1))\n",
    "    #Fixed\n",
    "    bq = bq_fid\n",
    "    alpha_z = alpha_z_fid#numpyro.sample(\"alpha_z\",dist.Uniform(0,2))#numpyro.deterministic('alpha_z',alpha_z_fid)#numpyro.sample(\"alpha_z\",dist.Normal(0,5))\n",
    "    zp = zp_fid #numpyro.deterministic('zp',zp_fid)\n",
    "    beta = beta_fid #numpyro.deterministic('beta',beta_fid)\n",
    "    bq = bq_fid #numpyro.sample(\"bq\",dist.Normal(0,5)) #when fitting m_2\n",
    "    \n",
    "    loglik, Neff = log_lik(h0,Om0,mmin,mmax,alpha,mu_m1,sig_m1,f_peak,bq,alpha_z,zp,beta)\n",
    "    \n",
    "    conv = numpyro.deterministic('conv', Neff/4/n_detections)\n",
    "\n",
    "    #Likelihood\n",
    "    numpyro.factor(\"logp\",loglik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nChains = 1\n",
    "numpyro.set_host_device_count(nChains)\n",
    "\n",
    "rng_key = random.PRNGKey(2)\n",
    "rng_key,rng_key_ = random.split(rng_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up NUTS sampler over our likelihood\n",
    "kernel = NUTS(log_probability)\n",
    "mcmc = MCMC(kernel,num_warmup=500,num_samples=1000,num_chains=nChains,chain_method='parallel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_details = '_Ndet_%s_Nsamples_%s_Nfoundinj_%s_Ninj_%s' % (n_detections,n_samples,Ndet,Ndraw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mcmc.run(rng_key_)#, init_params=jnp.array([H0_fid/100,Om0_fid,mmin_fid, mmax_fid,alpha_fid,mu_m1_fid,sig_m1_fid,f_peak_fid])\n",
    "mcmc.print_summary()\n",
    "samples = mcmc.get_samples()\n",
    "\n",
    "print('Fiducial values:')\n",
    "print('H0=',H0_fid,', Om0=',Om0_fid)\n",
    "print('alpha=',alpha_fid,', bq=',bq_fid,', f_peak=',f_peak_fid,', mmax=',mmax_fid,', mmin=',mmin_fid,', mu_m1=',mu_m1_fid,', sig_m1=',sig_m1_fid)\n",
    "print('alpha_z=',alpha_z_fid,', zp=',zp_fid,', beta=',beta_fid)\n",
    "az.plot_trace(mcmc, compact=True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(dir_plots+'plot_trace_'+model_name+inference_details+'.pdf',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "m1_grid = np.linspace(2,150,1000)\n",
    "\n",
    "random_inds = np.random.choice(np.arange(samples['mmax'].size),size=500)\n",
    "for i in random_inds:\n",
    "    \n",
    "    p_m1 = jgwpop.powerlaw_peak(m1_grid,\n",
    "                            samples['mmin'][i],\n",
    "                           samples['mmax'][i],\n",
    "                           samples['alpha'][i],\n",
    "                            samples['sig_m1'][i],\n",
    "                            samples['mu_m1'][i],\n",
    "                            samples['f_peak'][i])\n",
    "    p_m1 /= np.trapz(p_m1,m1_grid)\n",
    "    \n",
    "    ax.plot(m1_grid,p_m1,color='black',lw=0.5,alpha=0.5)\n",
    "\n",
    "massess = np.linspace(2,150,1000)\n",
    "pm_true = jgwpop.powerlaw_peak(massess,mmin_fid,mmax_fid,alpha_fid,sig_m1_fid,mu_m1_fid,f_peak_fid)\n",
    "pm_true /= jnp.trapz(pm_true,massess)\n",
    "\n",
    "ax.plot([],'k',alpha=0.5,label='Samples')\n",
    "ax.semilogy(massess,pm_true,label='True')    \n",
    "\n",
    "D_H = (Clight/1.0e3)  / H0_fid #Mpc\n",
    "m1z_mock_O5 = np.median(m1z_mock_samples,axis=1)\n",
    "m2z_mock_O5 = np.median(m2z_mock_samples,axis=1)\n",
    "dL_mock_O5 = np.median(dL_mock_samples,axis=1)\n",
    "m1s_mock, m2s_mock, zs_mock = jgwcosmo.detector_to_source_frame_approx_dLdH(m1z_mock_O5,m2z_mock_O5,dL_mock_O5/D_H,Om0_fid,zmin=1e-3,zmax=100)\n",
    "\n",
    "ax.hist(m1s_mock,bins=30,density=True, histtype='step',label='detected')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(1e-5,1)\n",
    "ax.set_xlim(0,100)\n",
    "plt.legend()\n",
    "plt.savefig(dir_plots+'pm_posteriors_'+model_name+inference_details+'.pdf',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = corner.corner(\n",
    "    samples,quantiles=[0.16, 0.5, 0.84],#, labels=labels\n",
    "                       show_titles=True,fontsize=fontsz#, truths=[H0_fid/100,alpha_ml, zp_ml]\n",
    ");\n",
    "plt.savefig(dir_plots+'corner_'+model_name+inference_details+'.pdf',bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inference_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
